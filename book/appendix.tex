
\appendix
\motto{Sin bravely.}
\chapter{Extra stuff that I hope might help}
\label{introA} % Always give a unique label

\section{Engineering Practices}
\label{sec:A1}
Over the years of doing engineering, we all come away with different learnings and a view of the task shaped by our experiences.
Below are some thoughts I have about engineering which have grow out of my experiences and condensed into topics. Perhaps some
of the topics will resonate with you and become part of your experience. The summary is below and details follow:
\begin{enumerate}
\item Define the problem. The problem you have isn't the problem you think you have.
\item Build in minimal commits.
\item If you don't understand all observations, then your understanding is wrong.
\item If code is not being used, then it doesn't work.
\item System problems can not be solved with single solutions, or by efforts at a single time.
\item Change one thing at a time.
\item There is no right way. There are only trade offs.
\item Build your personal rule set.
\item If you don't know what to do, then do something anyways.
\item Theory and practice.
\end{enumerate}

\subsection{Define the Problem.}
\label{sec:A2}
It should be axiomatic that if one cant define a problem, one cant solve it. It should be equally clear that if one defines a
problem incorrectly, then regardless of the quality of the solution one provides, the solution provides no value in the context of
the actual need.
As obvious as the above statements are, you should not deceive yourself into thinking that the ability to define
problems is natural, and even less that a problem, when it reaches your desk, has been correctly defined as stated. One should, in fact,
assume just the opposite.
In the course of your career, unless you are finding that 90 percent of the time your problems have to be reexamined and refined,
then it's probable you are not thinking carefully enough before you work.

Let's think through a current, discussion topic at Socotra, feature flags, to illustrate defining a problem. I don't happen to know what the
future resolution to the feature flag ``problem'' will be and perhaps that is good. You, the future reader with hindsight and proof of time, will
all the better be able to evaluate the usefulness of the thoughts on problem definition that follow.

Lets start with a real life, feature flag problem statement that we can quickly disregard.
\begin{description}
\item[Problem] The current feature flag
mechanism (with flags in database tables) is inelegant.
\end{description}
Perhaps no one has made exactly this feature flag problem statement. But statements like this have been thrown about and, of course as
developers, we love to disparage the technologically ugly with variants of this statement. Now, I am not sure what the official engineering
definition of inelegant is. If it had an engineering definition, I am not convinced that such a label would be sufficient to establish the
existence of a problem. Inelegant is an aesthetic, emotional judgment, and while there is room for aesthetics in engineering, we should be
on our guard. Feature flags presumably would fulfill a functional need and not an aesthetic, emotional need. Let's try again.

\begin{description}
\item[Problem] Lets investigate the Launch Darkly feature flag functionality to see how it could be used in our current systems.
\end{description}
This statement is an example of a solution with an assumed problem. A problem could exist, but certainly the author of the statement above
has skipped the formality mentioning it. As an engineer you should sensitize yourself to solutions that eclipse their problems.
``We should build new functionality as micro services'' or
``We should use Amazon Lambda for X, because Lambda is scalable and the future of software development'' are other examples. I am sure Launch Darkly is wonderful,
as well as micro services and Amazon Lambda, and yet their wonderfulness does not mandate their use. As an engineer, when technical discussion 
leads directly to technical implementation X or a technology Y, that is a tell that perhaps someone had a vague feeling about a
problem and that vague feeling of discontent was easiest for them to causally assign as due to the absence of a technology, for instance ``MongoDB''. (And as the joke goes, now
you have two problems).

Since we achieved insufficient clarity with the above problem definitions let's metaphorically go back to the beginning and talk about feature
flags. Feature flags are a soft deployment mechanism, which activate software functionality. Feature flags complement and add additional
flexibility to physical deployment mechanisms. In any existing implementation, the flag consists of a name and associated data (Boolean,
Enumerated...) stored in persistence somewhere. Is that elegant? We can list some of the common problems that feature flags solve. I took
the following problems from the web site of a commercial service that sells a feature flag management product. I believe the list is complete.

\begin{description}
  \item[Problem 1] A mechanism is needed to rollout software incrementally (for example: 10\% of users, 20\% of users,\dots) while monitoring
  metrics for errors or performance problems.
  \item[Problem 2] A short lived mechanism is needed to enable subsets of users to see different site functionality, perhaps for the purpose of
  running experiments.
  \item[Problem 3] A mechanism is needed to reconfigure code and it is not practical or desirable to do this via physical deployments.
\end{description}
Briefly reflect on how the above problems apply to Socotra. And with that reflection done, let's define Socotra's
problems.

Since feature flags are just a deployment mechanism, let's not immediately bring feature flags into our problem definition exercise, as we might
fool ourselves into
talking about a solution. Instead lets talk about what needs Socotra has around deployment functionality. Here is one deployment problem I
have witnessed at Socotra
\begin{description}
  \item[Problem] A mechanism is needed to rollout software incrementally (for example: organization \#1, a few days later organization \#2, \dots) while monitoring
  metrics for errors or performance problems.
\end{description}
For the instance where the above problem came up, Devops could have made incremental physical deployments but since the rollout had
more than one moving part it was conceptually simpler to put all the pieces in place and then flip a flag to bring everything live.
A database table, \_env\_feature, supplied the global, boolean feature flag to bring the new functionality live and that mechanism, in practice, turned out to be
simple and sufficient. There were no problems. That said, I can envision a more complex Socotra system where functionality might be
deployed in independent components in pieces; where one flag to bring all component functionality live at once, would enable conceptually simple deployment;
and where a flag in a database table would not be available to all the components. That would pose a deployment problem. We would probably
want to be aware of that future problem and address it at the proper time according to our engineering judgment.

In contrast, here is another common deployment problem I am aware of at Socotra
\begin{description}
  \item[Problem] New functionality X has been developed and insurance companies are dying to start using it. Unfortunately insurance companies
  cant readily use the new functionality because the new functionality requires that new fields or a new file be added to the product configuration. Even
  though insurance companies can deployed the new product configurations, all their existing policies will be tied to the previous product revision. They have to wait for each
  existing policy to be renewed, maybe a year, until their policyholders can all enjoy the new functionality. It's inconvenient for insurance
  companies, their systems, and their policyholders to receive functionality in this one policy by one policy  manner.
\end{description}
This is a problem that we anticipate having once product versioning goes live. Today this problem seems more immanent than any other deployment problem Socotra
has and as engineers we should have immanent problems clear in our minds. I am going to leave the possible importance of this problem and its solution to the future
but we might observe that practical engineering, perhaps not elegance and certainly not Launch Darkly, will be involved in the solution. We might also observe
that, exactly like feature flags, the deployment of configuration zip files is a type of soft deployment. We have an additional, existing soft deployment
capabilities which may not have been identified as such when we started the feature flag ``problem'' discussion.

I will leave the exercise there. There are probably more deployment problems that could be discussed. But that belabors the point to make
which is that
one should get into the habit of turning over a problem statement in ones mind, formulating it from various angles and reducing it to
essentials. In your career as an Engineer it is important that you be successful, and one of the keys skills that will guide you towards doing what should
be done, and assist co-workers with business skill sets is the ability to clearly and correctly define what the problem is.


\subsubsection{Build in minimal commits.}
I am often surprised, when I model a system with a computer checked tool, by how difficult it is to get the model correct. Usually the model
is highly abstracted and simplified, a toy compared to the real thing. And yet the tools find corner cases that I would never have discovered with
human reasoning skills. The reason for the surprise is that as engineers, and more so for non engineers, we have only a passing feeling for the
ability of state to multiply. Consider the simple, cancellation model in Chapter 2. How many states are there if the model steps through 8 time intervals?
There are almost a million and the model checker takes half and hour to check them all of them.

You are not impressed perhaps. We already know software is complex. This afternoon, I sat through an engineering discussion where an ongoing systems problem
was brought up:
``Its hard for me to predict the effects of my changes \dots'', because system bad. A few months ago I sat through a related engineering discussion about how
difficult it is to estimate delivery dates for new and ongoing projects, because system bad. What we need is? \- to have better encapsulation, to move the system
towards a micro-service architecture, to design with better architecture and patterns \dots or add your own favorite delusion. I am going to suggest an alternate theory for system
bad based on my frustrations building simple, abstract mathematical models. I will suggest that you are a very limited human being in the face of what you have created, and you have
to learn to work in a way that is appropriate to that reality.

Now, I know you are an experienced engineer, and you have read all the books, and you have come through the toughest, largest projects there are. You already
know what you are doing. And still there may be just one more practice that no one has ever told you and in fact its just the opposite of what the people who
talk cleaner code, and micro-service architecture, and design patterns propagandize. The practice is: make minimal commits. If you have a bug, or if you have
a feature to implement, or if you have a project to finish, find the smallest problem related to your bug, feature or project and define it precisely.
Write code to solve just that problem, precisely. Commit. Repeat until you are done. Anything can be accomplished as a series of minimal commits.

I know you will be tempted to disregard this practice. The minimum. That sounds so negligent. Why you may even desire to refactor. After all that was promoted in those
books about clean code, micro-services and patterns and stuff; the guys that wrote those books really know what they are doing, right? Well, I am not telling you to do the minimum. I am not telling
you to not make systems better. What I am saying is you are limited. You didn't 100\% understand the ramifications of that ambitious bug fix you made today and,
by the way, you didn't sleep well last night. That ambitious architecture idea you've been thinking on; two years from now you are going to look back and think
it was foolish, because you have grown. I am saying if you are limited, a precise, rigorous practice will help you be successful, and I am afraid other engineers
you meet in your career are not going to tell you. Minimal commits. Precisely define the minimal problem, write just that code, and commit. A lot of tasks
become better executed, a lot of systems improve, and you will be more successful if you can just repeat that process.

\subsection{If you don't understand all observations, then your understanding is wrong.}
One of the traits of a good theory is that when you apply it to a phenomena you are investigating, then it explains all of the observations you have made, not 90\%
of your observations, not 95\% of you observations, all of your observations. One of the traits of a bad theory is that it explains enough to
satisfy you. The classic example of a bad theory is Ptolemy's geocentric model of the universe. It explained almost all heavenly observations and served as the basis
for precise astronomical calculations. It was satisfactory enough. One just ignored the last few percent of observations that didn't make any sense. Now, stop for a
second and imagine yourself believing the geocentric model of the universe. Are you mostly correct in your understanding of the universe or do you know
essentially nothing about the universe?

If you have lived long enough, you can dip into your memories of past, very reasonable, theories which did not quite pan out. When I was growing up, I had an Aunt
who had a subscription to National Geographic, a very reputable magazine at the time. Sometime in the 70's I remember National Geographic running a whole magazine
detailing the case for an impending famine in the US. Back in the 70's it was also assured knowledge that the world would run out of oil sometime in the early 1990s.
In retrospect,
National Geographic knew essentially nothing about famine. Assured knowledge knew essentially nothing about the availability of energy resources on the planet. And yet
the theories accounted for most to the facts. How can that be? What you have to realize is that mismatches between observations and theories have to
drop your confidence in your theory a lot. If you have 10 observations and your theory explains 9 of those observations. As much as you'd like to,
you should not have 90\% confidence in your theory. You should have more like 50\% confidence. If you can explain 8 observations, you should have more like 25\%
confidence in your theory. Put another way you need more explanatory power than you think before you know more than nothing.

Now lets talk about theories in relation to computer systems at Socotra. I am going to recount a story from a few months ago. I was in Socotra's daily scrum and an engineer
explained that he was working on a bug. He explained that he couldn't quite reproduce the error or detail how the error was coming about. But he was pretty confident
that the ultimate cause had to be X and he had committed a solution Y. That is bad practice, and it is all too common among software engineers. Software engineering culture
accepts and allows people to make the jump from ``my theory explains an observation'' to ``my theory is correct''. This jump is not acceptable or allowed in any other
engineering discipline. Unless you can reproduce, unless you can explain, unless you can account for all the observations, then you know nothing. You need to
dig deeper, and you have no business committing solution Y, except to gain missing data.

Above we talked about when a bug is properly understood and hopefully corrected. On a related note, and getting back to the title of this post, imagine you are writing
code, building a system. Further, imagine that in one of the system executions you notice something curious. Possibly an error, but you cant reproduce it. And you are very busy;
it's probably something one off. I am going to suggest that your understanding of your code is like a theory and you have an observation that contradicts the
theory. With that one contradiction, you now need more explanatory power than you think to assume something one off happened. In fact, despite your
best feelings of assurance, you have to assume, with assurance, that the code is wrong. And do work to generate an explanation.

What I have written above might seem a bit strong. If it seems a bit strong then for your next few bugs fixes, start by listing all you theories. Maybe
you will start off with 10 possible theories and you might well add new theories as you work through the bug. Note that you will generally have to toss
away a lot of plausible theories before you finally find a root cause of your bug. Note also that the solution that you finally end with, will not generally be
in the initial set of theories you started with. What I have written above might, on the other hand, seem obvious. If that is that case, then the next time
you read the news just take note of topics of current concern and the related theories, sufficient for you to feel assured in your opinions. (Covid and Global
Warming are on the front page of CNN as I write this.) Then come back to this paragraph in 15 years.

\subsection{If code is not being used, then it doesn't work.}
When building systems its common to have unused code: sub-systems that are finished, waiting to go live or code that is not
executed on a regular basis. Any code that one expects to work must prove it works on a regular basis. If the code does not prove it works, your best assumption
is that it does not.

\subsection{System problems can not be solved with single solutions, or by efforts at a single time.}
In a recent engineering meeting development quality was evaluated in terms of the number of customer bug reports per month.
One tentative suggestion from the graphs was that there was a large number of latent bugs in existing code waiting to be exposed by
new customers doing new things. This prompted the question of how can we remove bugs from the existing code and the response was
``we have to write more tests''. How likely is that to be a solution for uncovering latent bugs in code?

We can answer questions like this by making a quick engineering estimates. How many state items are there in the configuration plus
database fields that can influence processing? Lets say the state items are bounded on the low side by $10^2$. Lets further assume
that all states are binary and
that processing is never influenced by the confluence of more than three data items. The system then has a lower bound of $10^6$ state combinations.
If not properly handling some of those $10^6$ states would lead to a bug, then we can say that to sufficiently
remove just data bugs, a lower limit on the number of tests necessary would be $10^6$. Further we would still have many other classes of bugs
to investigate with perhaps equally large state spaces. No team will ever write those tens of millions of tests, and if they could then they would
spend months of time just trying to run their test suite, once. The answer ``we have to write more tests'' does not work in theory. Slightly differently,
theory tells you a given solution is wrong.

I am going to give you another answer to the question of how to remove bugs in existing code. But the answer is general to all improvements
that one seeks to implement in complex systems. You have to accumulate a set of effective attack methods to use towards your goal and
you have to apply those methods systematically over an extended period of time. The extended period of time varies by the problem domain
but for software systems a typical value of the extended period of time is on the order of one year.

When I started my first job as a software engineer, I worked at a financial institution in Boston with a faulty reporting system. It was a big
problem because customers expected timely (right now) financial reports and there were ten to fifty failures that had to be resolved every morning. Every
morning I would try to identify every problem and manually produce the reports by any means necessary. Then I would go downstairs to my
desk and fix as many problems as were identified. I would also search all the other reports for any specific classes of problems that could be
extrapolated from the days failures and I would fix those too. For some issues I would ask ``what would I have to do to prevent an issue like
this from ever happening again?'' and then I would do that. For issues that I couldn't find a root cause for, I had a lab notebook where I
would record every issue and every observation that I had made about the issue. Everything was tracked. Where I needed more information,
I would add logging and monitoring. After a year of persistent attack from many angles, one day I came into work and there were no report
failures. Over the years, with many failing systems I have personally seen variations of the above story replay many times. There are no easy one
shot solutions. One applies many simple strategies, consistently over time, and in about a year one will end up where one wants to be.

\subsection{Change one thing at a time.}
Most systems, even simple systems are complex enough to exceed the reasoning ability of the best engineers, and we can see the manifestation of this truth regularly at
Socotra. We have a moderately complex system and yet its not unknown to see one or more engineers struggle to fix system problems and
be uncertain about the ultimate fix. Will the fix be correct in all circumstances? It's also not unknown for specifications of new functionality
to be incomplete or in need of revision after discussion of system details. Demonstrably, no one can reason about our system with confidence, and
I would suspect that the most confident are in line to fall the hardest.

One golden rule for navigating system complexity (along with minimal commits) is to always change one thing at a time. If someone changes module
X and you would like to add functionality in module X for your feature, Y. Wait for X to go live in production and then add your feature Y. If
problems occur in Module X or a related module before your changes land, then someone else is closer to isolating what changes have created problems.
It's always much harder to analyze a problem when multiple changes have happened at the same time especially if the changes interact in
unintuitive ways. One change at a time is also why many organizations deploy code continuously. If each commit is well targeted, which it should
be, then only one piece of functionality has changed. If after a deployment, monitors show abnormalities, then code can be rolled back and it will
be very apparent what single change caused what problem. Surprisingly what seems to be very dangerous, deploying as soon as commits land, turns
out to be safer because only one thing is changing at a time.

\subsection{There is no right way. There are only trade offs.}
Try never to say ``this is the right way to do it''. That statement may be emotionally satisfying but it is not engineering. Engineering
is about trade offs. Just like deciding on family priorities, or deciding what business opportunities to pursue.
One defines the problem (see above) or, mathematically, one defines a set of inequalities which bound the solution space. One then navigates among needs,
values, precise discussion with stakeholders, and hopefully good judgment to pick a point in that solution space. Note I said ``a point'' not
``the optimal point''. There is no optimal point as the optimal point depends on your loss function, and the loss function depends on ones priorities.
For some efficiency might be highly weighted in the loss function, for others time to market, or something else entirely, might be highly weighted. Optimality
depends on what trade offs are important to you. Being an engineer means you
can come up with many possible solutions to any problem and you can explain the trade off for each one.

\subsection{Build you personal rule set.}
When you work on complex systems try to find rules that can guide you in your interaction with that system. All interactions with complex systems
that have survived through the ages do precisely this. In human society the rules that guide interaction are called culture. In governance the
rules that guide interactions will be conventions like parliamentary procedures, rule of law, and freedom of speech. In an economic
system one rule might be voluntary exchange. The rules exist not because they are perfect but because they have been found to work well most
of the time. And because human judgment is not sound enough to distinguish the exceptional cases. Similarly when you code or modify systems think about the
rules that should guide you in your changes. If you create a bug perhaps you should have a rule for each bug you fix you must write a unit
test. That's a common rule that a lot of engineers have. If you have to make a change to a system you might decide that as a rule you will identify
the minimal fix, as discussed. And code just that. You might decide that you will never commit a change to source control without actually doing a
test on a live system and checking output, even when there is no chance you could have made a mistake.

Building a personal rule set is a task of identifying where you fail and where others succeed. In both cases you identify a better way that brings
you closer to the kind of engineer you want to be and then you stick to that way. You rules don't have to be perfect. They just need to lead you to do the
right thing most of the time. 

\subsection{If you don't know what to do, then do something anyways.}
In the course of you career you will have to solve many different kinds of problems. Some of those problems will be very stressful. They may
seem to be impossible to solve or understand. You may even run out of avenues of attack where you don't know what to try next. When this happens,
be systematic, don't give up, and do something anyways. Persisting and staying observant often leads to the discovery of a hint that will lead
you to your solution.

\subsection{Theory and Practice.}
In the course of my career I have met some very practical engineers who were very serious about doing but who lacked theory to
discipline their thoughts. Those engineers were handicapped without theory and they were often unaware of their handicap. I have also met very theoretical
engineers who lacked the applied interest in creating practical technical artifacts. Some of those engineers ended up disappointed and disillusioned,
when their lofty visions where not realized in practice.

Don't be a practitioner and don't be a theoretician. Your job as an engineer is to continuously circle back between both theory and practice. Theory tells
you which avenues will be possible and which will be impossible. It is your first error correction filter. Practice is where you meet
the real world and you learn the limitations of your technology and your abstractions through feedback. This is your second error correction filter. Once
you have met the real world then you recycle your learnings back into your theory to make it better. And you continue the cycle over and over.
